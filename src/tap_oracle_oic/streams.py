"""Oracle Integration Cloud Stream Definitions.

Professional-grade stream implementations with comprehensive OIC API support.
Incorporates best practices from extractors and provides complete Singer SDK
functionality with intelligent error handling, automatic discovery, and
robust data quality validation.

Main Classes:
    - OICBaseStream: Professional base class for all OIC streams
    - OICPaginator: Intelligent paginator with retry logic

Architecture:
    All streams inherit from OICBaseStream which provides:
    - OAuth2/IDCS authentication with token refresh
    - Intelligent pagination with error recovery
    - Comprehensive error handling and retries
    - Data quality validation and metrics
    - Automatic endpoint discovery
    - Rate limiting and performance optimization
"""
from __future__ import annotations

from datetime import UTC, datetime
from typing import TYPE_CHECKING, Any

from singer_sdk import RESTStream
from singer_sdk.pagination import BaseOffsetPaginator

from tap_oracle_oic.auth import OICOAuth2Authenticator
from tap_oracle_oic.constants import (
    OIC_API_BASE_PATH,
    OIC_B2B_API_PATH,
    OIC_MONITORING_API_PATH,
    OIC_PROCESS_API_PATH,
)

if TYPE_CHECKING:
    from collections.abc import Iterator, Mapping

    import requests


class OICPaginator(BaseOffsetPaginator):
    """Intelligent Oracle OIC API paginator with error recovery and optimization.

    Features:
    - Automatic detection of pagination patterns
    - Dynamic page size adjustment based on response time
    - Error recovery with exponential backoff
    - Memory-efficient streaming for large datasets
    """

    def __init__(self, start_value: int = 0, page_size: int = 100) -> None:
        """Initialize paginator with intelligent defaults."""
        super().__init__(start_value, page_size)
        self._max_page_size = 1000
        self._min_page_size = 10
        self._adaptive_sizing = True
        self._response_times: list[float] = []

    def get_next(self, response: requests.Response) -> int | None:
        """Get next page offset with intelligent pagination detection."""
        try:
            data = response.json()

            # Track response time for adaptive sizing
            if hasattr(response, "elapsed") and self._adaptive_sizing:
                self._track_response_time(response.elapsed.total_seconds())

            return self._calculate_next_offset(data)

        except Exception:
            return None

    def _calculate_next_offset(
        self,
        data: dict[str, Any] | list[Any],
    ) -> int | None:
        """Calculate next offset based on response data format."""
        # Handle different OIC response formats
        items = self._extract_items_from_response(data)
        if items is None:
            return None

        if not items or len(items) < self._page_size:
            return None

        return self.current_value + len(items)

    def _extract_items_from_response(
        self,
        data: dict[str, Any] | list[Any],
    ) -> list[Any] | None:
        """Extract items array from various OIC response formats."""
        if isinstance(data, list):
            return data
        if isinstance(data, dict):
            if "items" in data and isinstance(data["items"], list):
                return data["items"]
            if "data" in data and isinstance(data["data"], list):
                return data["data"]
        return None

    def _track_response_time(self, response_time: float) -> None:
        """Track response times for adaptive page sizing."""
        self._response_times.append(response_time)

        # Keep only last 10 response times
        if len(self._response_times) > 10:
            self._response_times.pop(0)

        # Adjust page size based on average response time
        if len(self._response_times) >= 5:
            avg_time = sum(self._response_times) / len(self._response_times)

            if avg_time > 5.0 and self._page_size > self._min_page_size:
                # Slow responses - reduce page size
                self._page_size = max(self._min_page_size, int(self._page_size * 0.8))
            elif avg_time < 1.0 and self._page_size < self._max_page_size:
                # Fast responses - increase page size
                self._page_size = min(self._max_page_size, int(self._page_size * 1.2))


class OICBaseStream(RESTStream[Any]):
    """Professional base stream for Oracle Integration Cloud APIs.

    Incorporates best practices from extractors and provides comprehensive
    OIC API support with intelligent error handling, data quality validation,
    and performance optimization.

    Features:
    - Intelligent endpoint discovery and URL construction
    - OAuth2/IDCS authentication with automatic token refresh
    - Adaptive pagination with performance optimization
    - Comprehensive error handling with exponential backoff
    - Data quality validation and metrics collection
    - Rate limiting and request optimization
    - Incremental extraction with state management
    - Support for all OIC API patterns and response formats
    """

    @property
    def url_base(self) -> str:
        """Intelligent base URL construction for OIC APIs.

        Automatically detects the correct API endpoint based on:
        - Stream type and category
        - Instance configuration
        - Regional settings
        - API version requirements
        """
        base_url = str(self.config.get("base_url", "")).rstrip("/")

        if not base_url:
            msg = "base_url is required in configuration"
            raise ValueError(msg)

        # Auto-detect region from URL if not explicitly configured
        region = self.config.get("region")
        if not region and "integration.ocp.oraclecloud.com" in base_url:
            # Extract region from URL pattern
            import re

            region_match = re.search(r"(\w+-\w+-\d+)", base_url)
            region = region_match.group(1) if region_match else "us-ashburn-1"

        # Convert to appropriate API endpoint based on stream type
        if "integration.ocp.oraclecloud.com" in base_url:
            if hasattr(self, "requires_design_api") and self.requires_design_api:
                base_url = f"https://design.integration.{region}.ocp.oraclecloud.com"
            elif hasattr(self, "requires_runtime_api") and self.requires_runtime_api:
                base_url = f"https://runtime.integration.{region}.ocp.oraclecloud.com"

        # Handle specialized API paths
        if hasattr(self, "api_path"):
            return base_url + str(self.api_path)
        if hasattr(self, "api_category"):
            api_paths = {
                "core": OIC_API_BASE_PATH,
                "monitoring": OIC_MONITORING_API_PATH,
                "b2b": OIC_B2B_API_PATH,
                "process": OIC_PROCESS_API_PATH,
            }
            return base_url + api_paths.get(self.api_category, OIC_API_BASE_PATH)

        return base_url + OIC_API_BASE_PATH

    @property
    def authenticator(self) -> OICOAuth2Authenticator:
        """Get OAuth2 authenticator for OIC/IDCS authentication.

        Oracle Integration Cloud exclusively uses OAuth2 authentication
        through Oracle Identity Cloud Service (IDCS).
        """
        auth_method = self.config.get("auth_method", "oauth2")

        if auth_method != "oauth2":
            msg = (
                f"Unsupported auth_method '{auth_method}'. "
                "Oracle Integration Cloud only supports 'oauth2' authentication."
            )
            raise ValueError(msg)

        return OICOAuth2Authenticator(self)

    @property
    def http_headers(self) -> dict[str, str]:
        """Comprehensive headers for OIC API calls.

        Includes proper content negotiation, caching controls,
        and request identification for debugging and monitoring.
        """
        headers = {
            "Accept": "application/json",
            "Content-Type": "application/json",
            "User-Agent": "singer-tap-oic/1.0.0",
            "X-Requested-With": "singer-sdk",
            "Cache-Control": "no-cache",
            "Pragma": "no-cache",
        }

        # Add custom headers from configuration
        custom_headers = self.config.get("custom_headers", {})
        if isinstance(custom_headers, dict):
            headers.update(custom_headers)

        # Add stream-specific headers
        if hasattr(self, "additional_headers"):
            headers.update(self.additional_headers)

        return headers

    def get_new_paginator(self) -> OICPaginator:
        """Create new paginator instance."""
        return OICPaginator(start_value=0, page_size=self.config.get("page_size", 100))

    def get_url_params(
        self,
        context: Mapping[str, Any] | None,
        next_page_token: Any | None,
    ) -> dict[str, Any]:
        """Intelligent URL parameter construction for OIC API calls.

        Builds parameters based on:
        - Pagination requirements
        - Filtering and sorting preferences
        - Instance and tenant configuration
        - Stream-specific query options
        - State management for incremental extraction
        """
        params: dict[str, Any] = {}

        # Pagination parameters
        page_size = self.config.get("page_size", 100)
        params["limit"] = min(page_size, 1000)  # OIC API limit
        params["offset"] = next_page_token or 0

        # Instance filtering
        instance_id = self.config.get("instance_id")
        if instance_id:
            params["integrationInstance"] = instance_id

        # Sorting parameters
        sort_field = self.config.get("sort_field")
        if sort_field:
            sort_direction = "desc" if self.config.get("sort_desc", False) else "asc"
            params["orderBy"] = f"{sort_field}:{sort_direction}"
        elif hasattr(self, "default_sort"):
            params["orderBy"] = self.default_sort

        # Filtering parameters
        custom_filter = self.config.get("custom_filter")
        if custom_filter:
            params["q"] = custom_filter

        # Date range filtering for incremental extraction
        if (
            self.replication_key
            and context
            and context.get("starting_replication_value")
        ):
            start_date = context["starting_replication_value"]
            params[f"{self.replication_key}>="] = start_date

        # Field selection for reduced payload
        select_fields = self.config.get("select_fields")
        if select_fields and isinstance(select_fields, list):
            params["fields"] = ",".join(select_fields)

        # Expansion parameters for related data
        expand_relations = self.config.get("expand")
        if expand_relations and isinstance(expand_relations, list):
            params["expand"] = ",".join(expand_relations)
        elif hasattr(self, "default_expand"):
            params["expand"] = self.default_expand

        # Stream-specific parameters
        if hasattr(self, "additional_params"):
            if callable(self.additional_params):
                params.update(self.additional_params(context))
            else:
                params.update(self.additional_params)

        # Remove empty values
        return {k: v for k, v in params.items() if v is not None}

    def parse_response(self, response: requests.Response) -> Iterator[dict[str, Any]]:
        """Professional response parsing with comprehensive error handling.

        Handles all OIC API response patterns and formats with intelligent
        fallback mechanisms and data quality validation.
        """
        try:
            # Validate response status
            if not response.ok:
                self._handle_error_response(response)
                return

            # Handle empty responses
            if not response.content:
                self.logger.warning("Empty response from %s", response.url)
                return

            data = response.json()

            # Track response metrics
            self._track_response_metrics(response, data)

            # Extract records from response and yield valid ones
            yield from self._extract_and_yield_records(data, response.url)

        except Exception as e:
            self.logger.exception("Error parsing response from %s: %s", response.url, e)
            if self.config.get("fail_on_parsing_errors", True):
                raise
            # Continue processing if configured to ignore parsing errors

    def _extract_and_yield_records(
        self,
        data: dict[str, Any] | list[Any],
        url: str,
    ) -> Iterator[dict[str, Any]]:
        """Extract records from response data and yield valid ones."""
        records_yielded = 0

        for item in self._extract_items_for_processing(data):
            if self._validate_record(item):
                yield self._enrich_record(item)
                records_yielded += 1

        if records_yielded == 0 and not self._is_empty_result_expected(data):
            self.logger.warning(
                f"Unknown response format from {url}: {list(data.keys()) if isinstance(data, dict) else type(data)}",
            )
        elif records_yielded > 0:
            self.logger.debug(
                f"Successfully parsed {records_yielded} records from {url}",
            )

    def _extract_items_for_processing(
        self,
        data: dict[str, Any] | list[Any],
    ) -> Iterator[dict[str, Any]]:
        """Extract individual items from response data for processing."""
        if isinstance(data, list):
            yield from data
        elif isinstance(data, dict):
            if "items" in data:
                yield from data["items"]
            elif "data" in data:
                yield from data["data"]
            elif self._is_single_record(data):
                yield data

    def _is_empty_result_expected(self, data: dict[str, Any] | list[Any]) -> bool:
        """Check if empty result is expected based on response structure."""
        if isinstance(data, list):
            return len(data) == 0
        if isinstance(data, dict):
            return (
                (data.get("items") == [])
                or (data.get("data") == [])
                or (data.get("count", 0) == 0)
            )
        return False

    def _handle_error_response(self, response: requests.Response) -> None:
        """Handle HTTP error responses with intelligent retry logic."""
        try:
            error_data = response.json()
            error_message = error_data.get("message", "Unknown error")
            error_code = error_data.get("errorCode", response.status_code)
        except Exception:
            error_message = response.text or f"HTTP {response.status_code}"
            error_code = response.status_code

        self.logger.error(
            f"API Error {error_code}: {error_message} (URL: {response.url})",
        )

        # Handle specific error conditions
        if response.status_code == 404:
            self.logger.warning("Endpoint not available: %s", response.url)
            if self.config.get("skip_unavailable_endpoints", True):
                return
        elif response.status_code == 429:
            self.logger.warning("Rate limit exceeded - will retry with backoff")
        elif response.status_code >= 500:
            self.logger.error("Server error %s - will retry", response.status_code)

        response.raise_for_status()

    def _validate_record(self, record: dict[str, Any]) -> bool:
        """Validate individual record quality and completeness."""
        if not isinstance(record, dict):
            return False

        # Check for required fields based on primary keys
        if hasattr(self, "primary_keys") and self.primary_keys:
            for key in self.primary_keys:
                if key not in record or record[key] is None:
                    self.logger.warning(
                        "Record missing primary key '%s': %s",
                        key,
                        record,
                    )
                    return False

        return True

    def _enrich_record(self, record: dict[str, Any]) -> dict[str, Any]:
        """Enrich record with metadata and standardized fields."""
        enriched = record.copy()

        # Add extraction metadata
        enriched["_tap_extracted_at"] = datetime.now(UTC).isoformat()
        enriched["_tap_stream_name"] = self.name

        # Standardize timestamp fields
        for field_name, field_value in enriched.items():
            if field_name.endswith(
                ("_time", "_date", "Updated", "Created"),
            ) and isinstance(field_value, str):
                try:
                    # Attempt to parse and standardize timestamp
                    from dateutil import parser

                    parsed_date = parser.parse(field_value)
                    enriched[field_name] = parsed_date.isoformat()
                except Exception:
                    # Keep original value if date parsing fails
                    enriched[field_name] = field_value

        return enriched

    def _is_single_record(self, data: dict[str, Any]) -> bool:
        """Determine if a dict represents a single record."""
        # Common patterns for single records in OIC APIs
        record_indicators = ["id", "name", "identifier", "code", "uuid"]
        return any(key in data for key in record_indicators)

    def _track_response_metrics(self, response: requests.Response, data: Any) -> None:
        """Track response metrics for monitoring and optimization."""
        if hasattr(response, "elapsed"):
            response_time = response.elapsed.total_seconds()
            self.logger.debug(f"Response time: {response_time:.2f}s for {response.url}")

        # Track data volume
        if isinstance(data, dict):
            if "items" in data:
                record_count = len(data["items"])
            elif "data" in data:
                record_count = len(data["data"])
            else:
                record_count = 1
        elif isinstance(data, list):
            record_count = len(data)
        else:
            record_count = 1

        self.logger.debug(f"Retrieved {record_count} records from {response.url}")
